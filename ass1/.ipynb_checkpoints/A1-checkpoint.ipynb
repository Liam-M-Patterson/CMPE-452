{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6f1ce7",
   "metadata": {},
   "source": [
    "# CISC/CMPE 452/COGS 400 Assignment 1 - Perceptron (10 points)  \n",
    "\n",
    "Please put your name and student id here\n",
    "\n",
    "    Liam Patterson, #20121376\n",
    "\n",
    "- The notebook file has clearly marked blocks where you are expected to write code. Do not write or modify any code outside of these blocks.\n",
    "- Make sure to restart and run all the cells from the beginning before submission. Do not clear out the outputs.\n",
    "- Mark will be deducted based on late policy (-1% of the course total marks per day after due date until the end date after which no assignments will be accepted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca32e4",
   "metadata": {},
   "source": [
    "### Build Model (6 points)  \n",
    "Implement **Simple Feedback Learning** for emotion classification (dataset from: https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp)\n",
    "\n",
    "Use the correct/incorrect feedback and info about (y>d) or (y<d) to change weights.  \n",
    "Refer to the **Perceptron slides**  \n",
    "\n",
    "- 1. Implement forward and calculate the output (2 point)  \n",
    "- 2. Update the weights and bias (2 points)  \n",
    "- 3. Predict function (1 point)  \n",
    "- 4. Activation function (1 point)  \n",
    "\n",
    "### Evaluator Function (2 point)  \n",
    "Implement the evaluator function with Pytorch or Numpy only   \n",
    "- Evaluation metrics include confusion matrix, accuracy, recall score, precision and F1 score\n",
    "\n",
    "### Train and Evaluate the Model (2 point)  \n",
    "Train the model with customized learning rate and number of iterations  \n",
    "Use the predict function to predict the labels with the test dataset  \n",
    "Evaluate the prediction results  \n",
    "- Evaluation metrics include confusion matrix, accuracy, recall score, precision and F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88fb434",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-30T15:55:22.461852Z",
     "iopub.status.busy": "2021-07-30T15:55:22.461145Z",
     "iopub.status.idle": "2021-07-30T15:55:22.477161Z",
     "shell.execute_reply": "2021-07-30T15:55:22.476528Z",
     "shell.execute_reply.started": "2021-07-30T15:51:46.068206Z"
    },
    "papermill": {
     "duration": 0.128227,
     "end_time": "2021-07-30T15:55:22.477337",
     "exception": false,
     "start_time": "2021-07-30T15:55:22.349110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec206866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T15:55:22.527612Z",
     "iopub.status.busy": "2021-07-30T15:55:22.527076Z",
     "iopub.status.idle": "2021-07-30T15:55:22.630770Z",
     "shell.execute_reply": "2021-07-30T15:55:22.630179Z",
     "shell.execute_reply.started": "2021-07-30T15:51:49.309549Z"
    },
    "papermill": {
     "duration": 0.129591,
     "end_time": "2021-07-30T15:55:22.630946",
     "exception": false,
     "start_time": "2021-07-30T15:55:22.501355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "sadness\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_train = pd.read_csv('data/train.txt', names=['Text', 'Emotion'], sep=';')\n",
    "df_test = pd.read_csv('data/test.txt', names=['Text', 'Emotion'], sep=';')\n",
    "\n",
    "print(df_train['Text'][1])\n",
    "print(df_train.Text[1])\n",
    "print(df_train['Emotion'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45b67a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Emotion\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = df_train['Text']\n",
    "y_train = df_train['Emotion']\n",
    "\n",
    "x_test = df_test['Text']\n",
    "y_test = df_test['Emotion']\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d9b94da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T15:55:22.734110Z",
     "iopub.status.busy": "2021-07-30T15:55:22.733595Z",
     "iopub.status.idle": "2021-07-30T15:55:23.506339Z",
     "shell.execute_reply": "2021-07-30T15:55:23.505894Z",
     "shell.execute_reply.started": "2021-07-30T15:51:50.435525Z"
    },
    "papermill": {
     "duration": 0.797748,
     "end_time": "2021-07-30T15:55:23.506460",
     "exception": false,
     "start_time": "2021-07-30T15:55:22.708712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         5362\n",
       "sadness     4666\n",
       "anger       2159\n",
       "fear        1937\n",
       "love        1304\n",
       "surprise     572\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Emotion.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf3c63",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9de64e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "15995    0\n",
      "15996    0\n",
      "15997    1\n",
      "15998    0\n",
      "15999    0\n",
      "Name: Emotion, Length: 16000, dtype: int64\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       1\n",
      "4       0\n",
      "       ..\n",
      "1995    0\n",
      "1996    0\n",
      "1997    1\n",
      "1998    1\n",
      "1999    0\n",
      "Name: Emotion, Length: 2000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# encode label\n",
    "y_train = y_train.replace({'joy':1, 'sadness':0, 'anger':0, 'fear':0, 'love':1, 'surprise':1})\n",
    "y_test = y_test.replace({'joy':1, 'sadness':0, 'anger':0, 'fear':0, 'love':1, 'surprise':1})\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e45acbde",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# We transform each text into a vector\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      6\u001b[0m x_test \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(x_test)\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2074\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2075\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2076\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2077\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2078\u001b[0m )\n\u001b[1;32m-> 2079\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2081\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5)#, stop_words='english')\n",
    "\n",
    "# We transform each text into a vector\n",
    "x_train = tfidf.fit_transform(x_train).toarray()\n",
    "x_test = tfidf.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dd38370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(y_test, y_pred):\n",
    "    ####################################################################################################\n",
    "    # enter code here to implement the evaluation matrices including confusion matrix, accuracy, precision and recall\n",
    "    # DO NOT use any python packages such as scikit-learn\n",
    "#     print(y_test)\n",
    "#     print(y_pred)\n",
    "    print\n",
    "    \n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    ####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ff2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedbackLearning(object):\n",
    "    def __init__(self):\n",
    "        self.history = {}\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['test_acc'] = []\n",
    "        \n",
    "    def f(self, x):\n",
    "        ####################################################################################################\n",
    "        # 4. enter code here to implement the activation function\n",
    "\n",
    "\n",
    "        ####################################################################################################\n",
    "        return fx\n",
    "    \n",
    "    def train(self, x, y, x_test, y_test, learning_rate=0.1, n_iters=10, verbose=True):\n",
    "        n_train, input_size = x.shape\n",
    "        n_test = x_test.shape[0]\n",
    "        # weight initialization\n",
    "        self.W = np.zeros(input_size)\n",
    "        self.b = np.zeros(1)\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            for xi, yi in zip(x, y):\n",
    "                # forward\n",
    "                ####################################################################################################\n",
    "                # 1. enter code here to calculate the output\n",
    "\n",
    "\n",
    "                ####################################################################################################\n",
    "\n",
    "                ####################################################################################################\n",
    "                # 2. enter code here to adjust the weights and bias\n",
    "\n",
    "\n",
    "                ####################################################################################################\n",
    "\n",
    "            train_acc = (self.predict(x) == y).sum() / n_train\n",
    "            test_acc = (self.predict(x_test) == y_test).sum() / n_test\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['test_acc'].append(test_acc)\n",
    "            if verbose:\n",
    "                print('epoch %d, train acc %.4f, test acc %.4f' % (i + 1, train_acc, test_acc))\n",
    "\n",
    "    def predict(self, x):\n",
    "        ####################################################################################################\n",
    "        # 3. enter code here to complete the predict function\n",
    "        # TODO: use the trained weights to predict labels and return the predicted labels\n",
    "\n",
    "\n",
    "        ####################################################################################################\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff486a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# enter code here to initialize and train the model\n",
    "\n",
    "\n",
    "####################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy\n",
    "plt.plot(model1.history['train_acc'], label='train_acc')\n",
    "plt.plot(model1.history['test_acc'], label='test_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b42d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# enter code here to evaluate the model with the evaluator function\n",
    "\n",
    "\n",
    "####################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 94.692615,
   "end_time": "2021-07-30T15:56:50.140202",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-30T15:55:15.447587",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
